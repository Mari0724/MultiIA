{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cea8382",
   "metadata": {},
   "source": [
    "# Módulo Automatización: Voz → Texto (Whisper)\n",
    "**Objetivo:** Implementar un micro-módulo reutilizable que reciba un archivo de audio (wav/mp3), lo transcriba a texto usando Whisper y devuelva la transcripción via API.  \n",
    "Arquitectura: **Hexagonal** (API / Application / Domain / Infrastructure).  \n",
    "Esto facilita cambiar el motor (Whisper → Google Speech → Azure) sin tocar la lógica de la aplicación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58da637",
   "metadata": {},
   "source": [
    "## Resumen rápido\n",
    "- **Entrada:** archivo de audio (POST /automation/voice-to-text).\n",
    "- **Salida:** JSON `{ \"transcription\": \"texto...\" }`.\n",
    "- **Capas:**\n",
    "  - `domain`: define el contrato (interface/protocol).\n",
    "  - `infrastructure`: implementación concreta (Whisper).\n",
    "  - `application`: lógica de la aplicación (usa la infraestructura).\n",
    "  - `api`: rutas FastAPI que exponen la funcionalidad.\n",
    "- **Requisitos:** Python 3.10+, pip, ffmpeg (para algunos formatos).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f37fe9",
   "metadata": {},
   "source": [
    "## Requisitos / Instalación\n",
    "En el entorno virtual (recomendado):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc3c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencias mínimas\n",
    "pip install fastapi uvicorn \"openai-whisper\" torch python-multipart\n",
    "# (Opcionales / para pruebas)\n",
    "pip install requests gTTS pydub\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c15509",
   "metadata": {},
   "source": [
    "**Notas**:\n",
    "- `openai-whisper` usa PyTorch; en CPU funciona pero será más lento. En máquinas con GPU instala la versión de `torch` adecuada (consulta la página oficial de PyTorch para el comando `pip` correcto).\n",
    "- Para procesar MP3/otros formatos `pydub` y `ffmpeg` pueden ser útiles. Instala `ffmpeg` en tu sistema si lo vas a usar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf088b9",
   "metadata": {},
   "source": [
    "## Estructura de archivos (dentro de `app/automation/`)\n",
    "app/\n",
    " └── automation/\n",
    "      ├── api/\n",
    "      │    └── routes.py\n",
    "      ├── application/\n",
    "      │    └── automation_service.py\n",
    "      ├── domain/\n",
    "      │    └── automation_interface.py\n",
    "      └── infrastructure/\n",
    "           └── whisper_engine.py\n",
    "Además deberías tener tu `main.py` donde incluyes el router (ya lo tienes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288256fd",
   "metadata": {},
   "source": [
    "### domain/automation_interface.py (código con comentario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Esta clase define una INTERFAZ usando `Protocol`, que funciona como un contrato.\n",
    "- `Protocol` en Python → permite declarar métodos que otras clases deben implementar,\n",
    "  parecido a las interfaces en Java o C#.\n",
    "- Aquí declaramos el método `transcribe(audio_path: str) -> str`, que recibe la ruta de un\n",
    "  archivo de audio y devuelve el texto transcrito.\n",
    "\n",
    "El `...` (ellipsis) significa:\n",
    "- \"Aquí no pongo implementación, solo marco que este método existe\".\n",
    "- Es equivalente (conceptualmente) a `pass`, pero se usa mucho en interfaces para dejar claro que\n",
    "  este espacio NO se llena aquí, sino en la clase que implemente la interfaz.\n",
    "- Sirve para que cualquier motor (Whisper, Google Speech, Azure, etc.) sepa qué función\n",
    "  debe tener y qué debe devolver, sin importar cómo lo haga internamente.\n",
    "\n",
    "Ventaja de esta separación:\n",
    "- La lógica de negocio (nuestro sistema necesita convertir voz a texto) no depende\n",
    "  de un motor específico.\n",
    "- La implementación (cómo se logra esa conversión) puede cambiar libremente.\n",
    "  Hoy usamos Whisper, mañana podríamos usar Google Speech, y no habría que modificar\n",
    "  el resto del código, porque todos cumplen el mismo contrato (`transcribe`).\n",
    "\"\"\"\n",
    "from typing import Protocol\n",
    "\n",
    "class VoiceToTextInterface(Protocol):\n",
    "    def transcribe(self, audio_path: str) -> str:\n",
    "        \"\"\"Transcribe un archivo de audio y devuelve texto\"\"\"\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3137589c",
   "metadata": {},
   "source": [
    "## Errores comunes\n",
    "- `ModuleNotFoundError: No module named 'whisper'` → revisa que `openai-whisper` esté instalado y Python esté usando el entorno correcto.\n",
    "- Problemas de memoria al cargar modelos grandes → usa \"base\" o \"tiny\" o ejecuta en máquina con GPU.\n",
    "- Audio no compatible / formato extraño → convierte a WAV (16k/44.1k) con `ffmpeg` o `pydub`.\n",
    "- Si la transcripción tarda mucho → prueba con modelos más pequeños o haz pruebas con audios cortos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8ede8",
   "metadata": {},
   "source": [
    "## Consideraciones para producción / concurso\n",
    "- **Modelo:** Para demo de concurso usa `base` (balance precisión/velocidad). Para mayor precisión en pruebas reales, `small` o `medium`.\n",
    "- **Escalado:** Cargar un modelo por proceso es costoso; usar un servicio dedicado (microservicio) con colas si habrá muchas solicitudes simultáneas.\n",
    "- **Persistencia:** Si deseas histórico, guarda transcripciones en una tabla `transcriptions` (id, original_filename, path_s3?, text, created_at, duration, confidence).\n",
    "- **Almacenamiento:** Para archivos grandes o producción, no guardes en disco local; usa S3 / MinIO y procesa desde allí.\n",
    "- **Seguridad:** Añadir autenticación/authorization (JWT) en las rutas.\n",
    "- **Observabilidad:** Añadir logging, métricas (Prometheus), seguimiento de latencias.\n",
    "- **Tolerancia a fallos:** manejo de timeouts, reintentos y circuit breakers si usas servicios externos.\n",
    "- **Costos:** modelos grandes consumen RAM/CPU. Prueba trade-off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05168d03",
   "metadata": {},
   "source": [
    "## Demo sugerida para el concurso\n",
    "1. Muestra Swagger: sube `test_audio.wav` y enseña la respuesta JSON.\n",
    "2. Haz un demo en vivo: graba un clip corto (teléfono) y súbelo.\n",
    "3. Explica la arquitectura hexagonal en una slide (domain/api/application/infrastructure).\n",
    "4. Muestra cómo cambiar la implementación (breve slide: \"si cambiamos Whisper por Google Speech, solo tocamos infraestructura\").\n",
    "5. Menciones técnicas: modelo usado, tiempo promedio de transcripción por audio de 10s (métricas).\n",
    "6. Siguiente pasos (facturas + recordatorios): explica que la capa `application` es donde guardaremos y conectaremos a la BD.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
